---
title: "Modelling exposure measurement error with Stan"
author: "Lachlan Cribb"
date: '2022-06-15'
slug: modelling-exposure-measurement-error-with-stan
categories: []
tags:
- bayes
- stan
subtitle: ''
summary: ''
authors: []
lastmod: '2022-06-15T19:48:43+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
header-includes:

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<div id="intro" class="section level3">
<h3>Intro</h3>
<p>Recently I’ve been trying to find alternatives to simply averaging repeatedly measured exposure variables, which seems to be typical in my field. After some discussions with some smart people, I was pointed to Stan as a possible way to improve on this. This post captures my exploration of this possibility.</p>
<p>Thanks to Richard McElreath for setting me on the Stan path and Andrew Johnson at the <a href="https://discourse.mc-stan.org/t/how-to-estimate-a-parameter-for-use-in-another-model-in-brms/27290/5">Stan forums</a> for helping me get my head around the code. Mistakes/confusions are of course my own.</p>
<p>Load packages:</p>
<pre class="r"><code>library(tidyverse)
library(cmdstanr)
library(posterior)</code></pre>
<p>Noisy data are simulated. x_no_error is the true exposure variable measured without error. x1 and x2 are noisy measurements of x_no_error. The outcome y is normally distributed. The regression coefficient of y on true_x is set to be 0.5. The aim here is to see how well different methods are able to recover that true coefficient. We compare three methods: </p>
<ol style="list-style-type: decimal">
<li>Simply averaging the replicates and using that estimated mean as a predictor of y </li>
<li>Simultaneously estimating the latent true value of x from the replicates and the effect of that latent true x on y using a Bayesian model. </li>
<li>The same model as (2) - though using a hierarchical prior for the latent true x. This way, the true x for each individual is shrunk towards the overall <em>population average</em> of true x, smoothing over the measurement error and producing a more reliable estimate of the effect of x on y. </li>
</ol>
<pre class="r"><code># simulate noisy x data

set.seed(999)

d &lt;- tibble(
  x_no_error = rnorm(300),
  y = 0.5*x_no_error + rnorm(300),
  # observed x values
  x1 = x_no_error + rnorm(300, sd = 1),
  x2 = x_no_error + rnorm(300, sd = 1))</code></pre>
<p>Add data to list for Stan:</p>
<pre class="r"><code>stan_data &lt;- list(
  N = nrow(d),
  Nmeasurements = ncol(d[,c(&quot;x1&quot;,&quot;x2&quot;)]),
  y = d$y,
  x = d[,c(&quot;x1&quot;,&quot;x2&quot;)]
)</code></pre>
</div>
<div id="model-1---averaged-replicates" class="section level3">
<h3>Model 1 - averaged replicates</h3>
<p>The first method, which is perhaps the most often used (and simplest), takes a simple average of the replicated measurements and uses that as data in the model for y. Here are the results for this method:</p>
<pre class="r"><code>d$mean_x &lt;- rowMeans(d[,3:4])

broom::tidy(lm(y ~ mean_x, data = d))</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  -0.0224    0.0568    -0.395 0.693       
## 2 mean_x        0.257     0.0469     5.47  0.0000000941</code></pre>
<p>By contrast, here is the estimated coefficient for the simulated x (measured without error):</p>
<pre class="r"><code>broom::tidy(lm(y ~ x_no_error, data = d))</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  0.00105    0.0554    0.0190 9.85e- 1
## 2 x_no_error   0.400      0.0574    6.98   1.95e-11</code></pre>
<p>The regression coefficient for mean_x, the average of the two x measurements, is clearly attenuated towards zero compared to the x variable measured without error. </p>
</div>
<div id="model-2---bayesian-model-with-flat-priors" class="section level3">
<h3>Model 2 - Bayesian model with flat priors</h3>
<p>Much like Model 1, in this model the mean (‘true’) value of x is estimated from the two replicates. However, in this case, because the true value of x is treated as a parameter (estimated with error), rather than as data, this model <em>should</em> (as I understand it) more accurately capture uncertainty in the relationship between x and y.</p>
<p>The model, in statistical notation, is:</p>
<p><span class="math display">\[
\begin{align*}
\text{x}_{obs,nj} &amp; \sim \mathcal N(\text{x}_{true,n} , \sigma_x) \\
\text{y}_n &amp; \sim \mathcal N(\alpha + \beta * \text{x}_{true,n}, \sigma_y)
\end{align*}
\]</span></p>
<p>All parameters have default (flat) priors. Here is the Stan code for the model:</p>
<pre class="stan"><code>
data {
  int N;                        // umber of individuals
  int Nmeasurements;            // number of replicates
  vector[N] y;                  // outcome
  matrix[N, Nmeasurements] x;   // N by Nmeasurements matrix x 
}

parameters {
  real&lt;lower=0&gt; sigma_x;    // sigma of latent true x
  vector[N] true_x;         // latent true x

  real&lt;lower=0&gt; sigma_y;    // sigma of y
  real alpha;               // intercept
  real beta;                // coefficient beta

}

model {

  for (n in 1:N) {
    x[n] ~ normal(true_x[n], sigma_x);        // model for true x
  }

  y ~ normal(alpha + beta * true_x, sigma_y); // model for y
}
</code></pre>
<p>Now estimate the model using cmdstanr.</p>
<pre class="r"><code>samp &lt;- latent_x_mod1$sample(
  data = stan_data,
  refresh = 0,
  iter_warmup = 1500,
  iter_sampling = 3000,
  parallel_chains = 4
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...
## 
## Chain 2 finished in 4.0 seconds.
## Chain 1 finished in 4.7 seconds.
## Chain 3 finished in 4.7 seconds.
## Chain 4 finished in 4.7 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 4.5 seconds.
## Total execution time: 4.9 seconds.</code></pre>
<p>Here are the estimates from the model.</p>
<pre class="r"><code>summarise_draws(samp$draws(c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma_x&quot;)))</code></pre>
<pre><code>## # A tibble: 3 × 10
##   variable    mean  median     sd    mad     q5    q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 alpha    -0.0228 -0.0231 0.0572 0.0579 -0.116 0.0712  1.00   18499.    8563.
## 2 beta      0.257   0.257  0.0476 0.0468  0.179 0.335   1.00   11849.    9078.
## 3 sigma_x   1.01    1.01   0.0422 0.0424  0.945 1.08    1.00    4609.    6990.</code></pre>
<p>Unexpectedly, these results are almost identical to those produced by simply treating the average of the replicates as data. Whether this holds with smaller sample sizes or under different conditions, I am not sure.</p>
<p>Nevertheless, the addition of reasonable priors as below improves things a lot.</p>
</div>
<div id="model-3---bayesian-model-with-hierarchical-priors" class="section level3">
<h3>Model 3 - Bayesian model with hierarchical priors</h3>
<p>In this model, a hierarchical prior is used for true_x. This way, as described earlier, the estimates of latent true x are pulled towards the overall estimated population distribution of true_x. In other words, the prior for the latent true x is learned from the data, adaptively shrinking the each latent true x, especially those which are wayward or highly uncertain, towards its overall population average</p>
<p>The model, including priors, is:</p>
<p><span class="math display">\[
\begin{align*}
\text{x}_{obs,nj} &amp; \sim \mathcal N(\text{x}_{true,n} , \sigma_x) \\
\text{y}_n &amp; \sim \mathcal N(\alpha + \beta * \text{x}_{true,n}, \sigma_y)
\end{align*}
\]</span></p>
<p>Weakly informative priors are used for all coefficients. A hierarchical prior is used for the true latent x:</p>
<p><span class="math display">\[
\begin{align*}
\text{x}_{true,n} &amp; \sim \mathcal N(\mu_{xtrue}, \sigma_{xtrue}) \\
\mu_{xtrue} &amp; \sim \mathcal N(0,1) \\
\sigma_{xtrue} &amp; \sim exponential(1) \\
\alpha &amp; \sim \mathcal N(0,1) \\
\beta &amp; \sim \mathcal N(0,1) \\
\sigma_x &amp; \sim ~ exponential(1) \\
\sigma_y &amp; \sim ~ exponential(1)
\end{align*}
\]</span></p>
<p>Here is the Stan code for the model:</p>
<pre class="stan"><code>
data {
  int N;                        // number of individuals
  int Nmeasurements;            // number of replicates
  vector[N] y;                  // outcome
  matrix[N, Nmeasurements] x;   // N by Nmeasurements matrix x 
}

parameters {
  real&lt;lower=0&gt; sigma_x;    // sigma of latent true x
  vector[N] true_x;         // latent true x
  real mu_truex;            // location of prior on true x
  real&lt;lower=0&gt; sd_truex;   // scale of prior on true x

  real&lt;lower=0&gt; sigma_y;    // sigma of y
  real alpha;               // intercept
  real beta;                // coefficient beta

}

model {
  true_x ~ normal(mu_truex, sd_truex);   // prior on true x
  mu_truex ~ normal(0,1);                // hyperprior
  sd_truex ~ exponential(1);             // hyperprior
  sigma_x ~ exponential(1);              // prior on sigma_x
  
  alpha ~ normal(0,1);           // intercept prior
  beta ~ normal(0,1);            // beta prior
  sigma_y ~ exponential(1);      // prior on sigma_y

  for (n in 1:N) {
    x[n] ~ normal(true_x[n], sigma_x);        // model for true x
  }

  y ~ normal(alpha + beta * true_x, sigma_y); // model for y
}
</code></pre>
<p>Now estimate the model using cmdstanr.</p>
<pre class="r"><code>samp2 &lt;- latent_x_mod2$sample(
  data = stan_data,
  refresh = 0,
  iter_warmup = 1500,
  iter_sampling = 3000,
  parallel_chains = 4
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...
## 
## Chain 1 finished in 4.9 seconds.
## Chain 3 finished in 4.9 seconds.
## Chain 2 finished in 5.0 seconds.
## Chain 4 finished in 5.0 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 4.9 seconds.
## Total execution time: 5.0 seconds.</code></pre>
<p>Here are the estimates from this model using a hierarchical prior. Also presented are the estimated mean and standard deviation of the population true_x (i.e., the prior for each individual’s true_x, which was estimated from the data).</p>
<pre class="r"><code>summarise_draws(samp2$draws(c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma_x&quot;, &quot;mu_truex&quot;, &quot;sd_truex&quot;)))</code></pre>
<pre><code>## # A tibble: 5 × 10
##   variable    mean  median     sd    mad     q5    q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 alpha    -0.0127 -0.0128 0.0589 0.0587 -0.109 0.0840  1.00   20823.    8163.
## 2 beta      0.400   0.399  0.0780 0.0775  0.276 0.530   1.00    7613.    8076.
## 3 sigma_x   1.01    1.01   0.0421 0.0420  0.948 1.09    1.00    6886.    8854.
## 4 mu_truex -0.0647 -0.0652 0.0690 0.0698 -0.178 0.0489  1.00   13459.    9752.
## 5 sd_truex  0.971   0.971  0.0652 0.0639  0.866 1.08    1.00    5361.    6270.</code></pre>
<p>This model seems to do much better than the two previous. The estimated regression coefficient is much closer to that of x_no_error.</p>
<p>Compare with regression using the x_no_error variable and one using a simple average of the replicated measurements:</p>
<pre class="r"><code>broom::tidy(lm(y ~ x_no_error, data = d))</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  0.00105    0.0554    0.0190 9.85e- 1
## 2 x_no_error   0.400      0.0574    6.98   1.95e-11</code></pre>
<pre class="r"><code>broom::tidy(lm(y ~ mean_x, data = d))</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  -0.0224    0.0568    -0.395 0.693       
## 2 mean_x        0.257     0.0469     5.47  0.0000000941</code></pre>
<p>In conclusion, fitting a hierarchical model in Stan for a noisy x variable measured in replicates can produce much more accurate inferences. How these results extend to more complex situations involving missing data and other complexities, remains to be seen.</p>
</div>
