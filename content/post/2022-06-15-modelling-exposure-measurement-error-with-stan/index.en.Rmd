---
title: "Modelling exposure measurement error with Stan"
author: "Lachlan Cribb"
date: '2022-06-15'
slug: modelling-exposure-measurement-error-with-stan
categories: []
tags:
- bayes
- stan
subtitle: ''
summary: ''
authors: []
lastmod: '2022-06-15T19:48:43+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
header-includes:

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

---

### Intro

Recently I've been trying to find alternatives to simply averaging repeatedly measured exposure variables, which seems to be typical in my field. After some discussions with some  smart people, I was pointed to Stan as a possible way to improve on this. This post captures my exploration of this possibility.

Thanks to Richard McElreath for setting me on the Stan path and Andrew Johnson at the [Stan forums](https://discourse.mc-stan.org/t/how-to-estimate-a-parameter-for-use-in-another-model-in-brms/27290/5) for helping me get my head around the code. Mistakes/confusions are of course my own. 



Load packages:

```{r setup, warning = F, message = F}
library(tidyverse)
library(cmdstanr)
library(posterior)
```

```{r setup2, include = F, echo = F}

register_knitr_engine()

```

Noisy data are simulated. x_no_error is the true exposure variable measured without error. x1 and x2 are noisy measurements of x_no_error. The outcome y is normally distributed. The regression coefficient of y on true_x is set to be 0.5. The aim here is to see how well different methods are able to recover that true coefficient. We compare three methods: \n

1) Simply averaging the replicates and using that estimated mean as a predictor of y \n
2) Simultaneously estimating the latent true value of x from the replicates and the effect of that latent true x on y using a Bayesian model. \n
3) The same model as (2) - though using a hierarchical prior for the latent true x. This way, the true x for each individual is shrunk towards the overall _population average_ of true x, smoothing over the measurement error and producing a more reliable estimate of the effect of x on y. \n



```{r simulate_data}

# simulate noisy x data

set.seed(999)

d <- tibble(
  x_no_error = rnorm(300),
  y = 0.5*x_no_error + rnorm(300),
  # observed x values
  x1 = x_no_error + rnorm(300, sd = 1),
  x2 = x_no_error + rnorm(300, sd = 1))

```

Add data to list for Stan: 

```{r}

stan_data <- list(
  N = nrow(d),
  Nmeasurements = ncol(d[,c("x1","x2")]),
  y = d$y,
  x = d[,c("x1","x2")]
)


```

### Model 1 - averaged replicates ###

The first method, which is perhaps the most often used (and simplest), takes a simple average of the replicated measurements and uses that as data in the model for y. Here are the results for this method: 

```{r compare}

d$mean_x <- rowMeans(d[,3:4])

broom::tidy(lm(y ~ mean_x, data = d))

```

By contrast, here is the estimated coefficient for the simulated x (measured without error): 

```{r no error}

broom::tidy(lm(y ~ x_no_error, data = d))

```

The regression coefficient for mean_x, the average of the two x measurements, is clearly attenuated towards zero compared to the x variable measured without error. \n

### Model 2 - Bayesian model with flat priors ###

Much like Model 1, in this model the mean ('true') value of x is estimated from the two replicates. However, in this case, because the true value of x is treated as a parameter (estimated with error), rather than as data, this model _should_ (as I understand it) more accurately capture uncertainty in the relationship between x and y.

The model, in statistical notation, is: 

$$ 
\begin{align*}
\text{x}_{obs,nj} & \sim \mathcal N(\text{x}_{true,n} , \sigma_x) \\
\text{y}_n & \sim \mathcal N(\alpha + \beta * \text{x}_{true,n}, \sigma_y) 
\end{align*}
$$

All parameters have default (flat) priors. Here is the Stan code for the model:


```{stan specify_model1, output.var = "latent_x_mod1"}

data {
  int N;                        // umber of individuals
  int Nmeasurements;            // number of replicates
  vector[N] y;                  // outcome
  matrix[N, Nmeasurements] x;   // N by Nmeasurements matrix x 
}

parameters {
  real<lower=0> sigma_x;    // sigma of latent true x
  vector[N] true_x;         // latent true x

  real<lower=0> sigma_y;    // sigma of y
  real alpha;               // intercept
  real beta;                // coefficient beta

}

model {

  for (n in 1:N) {
    x[n] ~ normal(true_x[n], sigma_x);        // model for true x
  }

  y ~ normal(alpha + beta * true_x, sigma_y); // model for y
}

```

Now estimate the model using cmdstanr.

```{r estimate_model1, message = F, warning = F, cache=F}

samp <- latent_x_mod1$sample(
  data = stan_data,
  refresh = 0,
  iter_warmup = 1500,
  iter_sampling = 3000,
  parallel_chains = 4
)
```

Here are the estimates from the model. 


```{r estimates1}

summarise_draws(samp$draws(c("alpha", "beta", "sigma_x")))


```

Unexpectedly, these results are almost identical to those produced by simply treating the average of the replicates as data. Whether this holds with smaller sample sizes or under different conditions, I am not sure. 

Nevertheless, the addition of reasonable priors as below improves things a lot. 

### Model 3 - Bayesian model with hierarchical priors ###

In this model, a hierarchical prior is used for true_x. This way, as described earlier, the estimates of latent true x are pulled towards the overall estimated population distribution of true_x. In other words, the prior for the latent true x is learned from the data, adaptively shrinking the each latent true x, especially those which are wayward or highly uncertain, towards its overall population average

The model, including priors, is:

$$ 
\begin{align*}
\text{x}_{obs,nj} & \sim \mathcal N(\text{x}_{true,n} , \sigma_x) \\
\text{y}_n & \sim \mathcal N(\alpha + \beta * \text{x}_{true,n}, \sigma_y)
\end{align*}
$$

Weakly informative priors are used for all coefficients. A hierarchical prior is used for the true latent x:

$$
\begin{align*}
\text{x}_{true,n} & \sim \mathcal N(\mu_{xtrue}, \sigma_{xtrue}) \\
\mu_{xtrue} & \sim \mathcal N(0,1) \\
\sigma_{xtrue} & \sim exponential(1) \\
\alpha & \sim \mathcal N(0,1) \\
\beta & \sim \mathcal N(0,1) \\
\sigma_x & \sim ~ exponential(1) \\
\sigma_y & \sim ~ exponential(1)
\end{align*}
$$


Here is the Stan code for the model: 

```{stan specify_model2, output.var = "latent_x_mod2"}

data {
  int N;                        // number of individuals
  int Nmeasurements;            // number of replicates
  vector[N] y;                  // outcome
  matrix[N, Nmeasurements] x;   // N by Nmeasurements matrix x 
}

parameters {
  real<lower=0> sigma_x;    // sigma of latent true x
  vector[N] true_x;         // latent true x
  real mu_truex;            // location of prior on true x
  real<lower=0> sd_truex;   // scale of prior on true x

  real<lower=0> sigma_y;    // sigma of y
  real alpha;               // intercept
  real beta;                // coefficient beta

}

model {
  true_x ~ normal(mu_truex, sd_truex);   // prior on true x
  mu_truex ~ normal(0,1);                // hyperprior
  sd_truex ~ exponential(1);             // hyperprior
  sigma_x ~ exponential(1);              // prior on sigma_x
  
  alpha ~ normal(0,1);           // intercept prior
  beta ~ normal(0,1);            // beta prior
  sigma_y ~ exponential(1);      // prior on sigma_y

  for (n in 1:N) {
    x[n] ~ normal(true_x[n], sigma_x);        // model for true x
  }

  y ~ normal(alpha + beta * true_x, sigma_y); // model for y
}

```

Now estimate the model using cmdstanr.

```{r estimate_model2, message = F, warning = F, cache=F}

samp2 <- latent_x_mod2$sample(
  data = stan_data,
  refresh = 0,
  iter_warmup = 1500,
  iter_sampling = 3000,
  parallel_chains = 4
)
```

Here are the estimates from this model using a hierarchical prior. Also presented are the estimated mean and standard deviation of the population true_x (i.e., the prior for each individual's true_x, which was estimated from the data).


```{r estimates2}

summarise_draws(samp2$draws(c("alpha", "beta", "sigma_x", "mu_truex", "sd_truex")))


```

This model seems to do much better than the two previous. The estimated regression coefficient is much closer to that of x_no_error. 

Compare with regression using the x_no_error variable and one using a simple average of the replicated measurements: 

```{r compare2}

broom::tidy(lm(y ~ x_no_error, data = d))

broom::tidy(lm(y ~ mean_x, data = d))

```

In conclusion, fitting a hierarchical model in Stan for a noisy x variable measured in replicates can produce much more accurate inferences. How these results extend to more complex situations involving missing data and other complexities, remains to be seen. 