---
title: "A simple alternative to averaging replicated exposure measurements"
author: "Lachlan Cribb"
date: '2022-07-15'
slug: An-alternative-to-averaging-replicated-exposure-measurements
categories: []
tags:
- regression
- hierarchical models
subtitle: ''
summary: ''
authors: []
lastmod: '2022-07-15T19:48:43+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
header-includes: <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<!-- ### Introduction -->
<p>Exposure variables are routinely measured in replicates in many medical research fields. In my field of epidemiology, for instance, blood biomarkers are often measured in duplicate. These variables are measured in replicates because their measurement is subject to considerable technical and/or biological variability and by performing repeated measurements we can hopefully get a better picture of the latent ‘true’ level of the exposure.</p>
<p>The problem is that it is not straightforward to include replicated exposure measurements as a predictor term in a regression model. So, when one wishes to use the exposure in a regression context, it is common for the replicated measurements to be averaged first. The resulting average is then used as a predictor in subsequent regression models. This averaging smooths away some noise and outliers - giving hopefully a more accurate picture of the latent level of the exposure variable. While simple and intuitive, there is at least one problem with this approach: there is no ‘borrowing of information’. That is, the data from other individuals in the study is not used in any way to inform the latent exposure level of another.</p>
<p>A potential improvement on this could be to use a hierarchical model to estimate each subjects latent exposure level. With this model, we would take advantage of partial pooling to adaptively ‘shrink’ each participants latent exposure level towards the overall sample average, particularly if their observations are quite extreme.</p>
<p>First, I briefly describe the two approaches and how they relate to eachother.</p>
<div id="approach-1-simple-averaging-of-replicates" class="section level4">
<h4>Approach 1: simple averaging of replicates</h4>
<p>To see clearly how the averaging of replicates relates to the hierarchical model described later, consider the below linear model. Note that the predictions provided by this linear model are averaged replicates for each individual. In other words, this model estimates the latent exposure level for each participant simply by averaging their replicated observations.</p>
<p><span class="math display">\[
\begin{align*}
\text{x}_{ij} &amp; \sim \mathcal N(\mu_{ij}, \sigma) \\
\mu     _{ij} &amp; = \beta_1 \text{ID1}_{ij} + \beta_2 \text{ID2}_{ij} + \beta_3 \text{ID3}_{ij}... + \beta_n \text{IDn}_{ij}
\end{align*}
\]</span>
Where <span class="math inline">\(\text{x}_{ij}\)</span> is the exposure measurement for the <span class="math inline">\(j^{th}\)</span> replicate for the <span class="math inline">\(i^{th}\)</span> participant, and <span class="math inline">\(\mu_{it}\)</span> represents the average exposure level for a given ID (participant). Notice that no information is shared between participants (each participant has their own intercept in the model). In other words, this model produces the so-called ‘no pooling’ estimate described by <a href="https://vulstats.ucsd.edu/pdf/Gelman.ch-12.basic-multilevel-models.pdf">Gelman and Hill</a>.</p>
</div>
<div id="approach-2-hierachical-model" class="section level4">
<h4>Approach 2: hierachical model</h4>
<p>In the second approach, an overall global mean (intercept) is estimated and participant-level deviations from that overall mean are represented by an error term (random intercept):</p>
<p><span class="math display">\[
\begin{align*}
x_{ij} &amp; \sim \mathcal N(\mu_{ij}, \sigma_e) \\
\mu     _{ij} &amp; = \beta_0 + u_{0i} \\
u_{0i} &amp; \sim \mathcal N(0, \sigma_0) \\
\end{align*}
\]</span></p>
<p>Alternatively, in lme4 syntax, the model is x ~ 1 + (1 | ID).
In this model, the estimated latent exposure level for each participant is a <em>compromise between the overall sample average <span class="math inline">\(\beta_0\)</span> and the no pooling estimate above</em>. How much each estimate is pulled closer to the overall mean (how much ‘partial pooling’) is determined by the standard deviation of the random intercept term <span class="math inline">\(\sigma_0\)</span>. As an extreme case, when <span class="math inline">\(\sigma_0\)</span> = 0, the estimate for each participant is simply the sample average. As <span class="math inline">\(\sigma_0\)</span> approaches <span class="math inline">\(\infty\)</span>, the no pooling estimate is returned (resulting in identical averages to those estimated from the first model described above). Partial pooling uses the data to identify the best compromise between those extremes.</p>
</div>
<div id="simulation" class="section level3">
<h3>Simulation</h3>
<p>For this post, I will see how these two methods compare in a simulation study under a few different scenarios. In the simulation, the regression coefficient for the latent ‘true’ exposure and outcome relationship will be set to 0.50, and I will determine how well each of the two methods are able to reproduce that correct regression coefficient.</p>
<p>First, to demonstrate the differences between the approaches, I will create a fake dataset (n = 100) to test them both. The intraclass correlation (ICC) of the replicated exposure measurements is set to 0.65.</p>
<pre class="r"><code># load packages
library(tidyverse)
library(lme4)
library(broom)
library(ggdist)
library(bayesplot)


# create dataset 
d &lt;- 
  tibble(latent_x = rnorm(100),
         y = 0.5*latent_x + rnorm(100),
         # observed x values
         x_obs1 = latent_x + rnorm(100, sd = 0.75),
         x_obs2 = latent_x + rnorm(100, sd = 0.75),
         ID = seq(1:100))
  
# approach 1: average x
d$average_x &lt;- rowMeans(d[,3:4])
  
# approach 2: hierarchical model
  
dlong &lt;- d %&gt;% 
  pivot_longer(c(x_obs1,x_obs2), values_to = &quot;x_obs&quot;)

hmod &lt;- lmer(x_obs ~ 1 + (1 | ID), data = dlong)  
  
d$x_hmod &lt;- predict(hmod, newdata = d)</code></pre>
<p>This plot demonstrates that the estimated exposure level from the hierarchical model (x_hmod) are shrunk towards the overall sample mean, relative to the no-pooling estimate (average_x).</p>
<pre class="r"><code>d %&gt;% 
  slice(1:25) %&gt;% 
  pivot_longer(c(average_x,x_hmod),
               values_to = &quot;x&quot;, names_to = &quot;method&quot;) %&gt;% 
  ggplot(aes(x = x, y = ID, colour = method, group = ID)) +
  geom_point() +
  geom_vline(aes(xintercept = 0)) +
  geom_line() +
  theme_default()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/shrinkage_plot-1.png" width="672" />
Note that t no pooling (average_x) estimates which are quite extreme are more aggressively pooled towards the overall mean than those which relatively more mean adjacent. The hierarchical model is using the data from the whole sample and in doing so determines that those particularly extreme observations are not really to be trusted.</p>
<p>To perform the simulation, I create a function to simulate data and extract the results of fitting the substantive model (y ~ x) using each approach.</p>
<pre class="r"><code>sim_data &lt;- function(SEED, n, noise_sd){
  
  set.seed(SEED)
  
  d &lt;- tibble(
  latent_x = rnorm(n),
  y = 0.5*latent_x + rnorm(n),
  # observed x values
  x_obs1 = latent_x + rnorm(n, sd = noise_sd),
  x_obs2 = latent_x + rnorm(n, sd = noise_sd),
  ID = seq(1:n))
  
  d$average_x &lt;- rowMeans(d[,3:4])
  
  # hierarchical model
  
  dlong &lt;- d %&gt;% 
    pivot_longer(c(x_obs1,x_obs2), values_to = &quot;x_obs&quot;)

  hmod &lt;- lmer(x_obs ~ 1 + (1 | ID), data = dlong)  
  
  d$x_hmod &lt;- predict(hmod, newdata = d)
  
  results &lt;- 
    bind_rows(est_avg = broom::tidy(lm(y ~ average_x, data = d), conf.int = T)[2,],
              est_hmod = broom::tidy(lm(y ~ x_hmod, data = d), conf.int = T)[2,])
  
  results
}</code></pre>
<p>Now, the simulation is performed for a combination of sample sizes (25, 50, 200) and ICC (0.5, 0.65, 0.80). The latter representing quite poor, reasonable, and good reliability, respectively.</p>
<pre class="r"><code># Perform simulation

# ICC = 0.5

out_0.5 &lt;- 
  expand_grid(SEED = 1:2000, n = c(25, 50, 200)) %&gt;% 
  mutate(res = map2(SEED, n, sim_data, noise_sd = 1)) %&gt;% 
  unnest(res) %&gt;% 
  mutate(ICC = 0.5)


# ICC = 0.65

out_0.65 &lt;- 
  expand_grid(SEED = 1:2000, n = c(25, 50, 200)) %&gt;% 
  mutate(res = map2(SEED, n, sim_data, noise_sd = 0.75)) %&gt;% 
  unnest(res) %&gt;% 
  mutate(ICC = 0.65)

# ICC = 0.8

out_0.8 &lt;- 
  expand_grid(SEED = 1:2000, n = c(25, 50, 200)) %&gt;% 
  mutate(res = map2(SEED, n, sim_data, noise_sd = 0.5)) %&gt;% 
  unnest(res) %&gt;% 
  mutate(ICC = 0.8)</code></pre>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<div id="bias" class="section level4">
<h4>Bias</h4>
<p>Bias in the estimated regression coefficient for the relationship between exposure x and outcome y is presented first for small sample sizes (n = 25).</p>
<pre class="r"><code>bind_rows(out_0.5, out_0.65, out_0.8) %&gt;% 
  filter(n == 25) %&gt;% 
  mutate(n = str_c(&quot;n==&quot;, n),
         ICC = str_c(&quot;ICC ==&quot;, ICC)) %&gt;% 
  ggplot(aes(x = estimate, y = term)) +
  stat_dotsinterval(.width = .5, slab_shape = 22) +
  facet_wrap(vars(n, ICC), labeller = label_parsed) +
  geom_vline(aes(xintercept = 0.5)) +
  scale_slab_fill_continuous() +
  scale_slab_color_continuous() +
  coord_cartesian(ylim = c(1.4, NA)) +
  xlim(c(-0.5,1.5)) +
  labs(x = &quot;Estimated regression coefficient&quot;,
       y = &quot;Method&quot;) +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = &quot;none&quot;) +
  theme_default()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/bias_1-1.png" width="672" /></p>
<p>Medium sample size (n = 50)</p>
<pre class="r"><code>bind_rows(out_0.5, out_0.65, out_0.8) %&gt;% 
  filter(n == 50) %&gt;% 
  mutate(n = str_c(&quot;n==&quot;, n),
         ICC = str_c(&quot;ICC ==&quot;, ICC)) %&gt;% 
  ggplot(aes(x = estimate, y = term)) +
  stat_dotsinterval(.width = .5, slab_shape = 22) +
  facet_wrap(vars(n, ICC), labeller = label_parsed) +
  geom_vline(aes(xintercept = 0.5)) +
  scale_slab_fill_continuous() +
  scale_slab_color_continuous() +
  coord_cartesian(ylim = c(1.4, NA)) +
  xlim(c(0,1)) +
  labs(x = &quot;Estimated regression coefficient&quot;,
       y = &quot;Method&quot;) +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = &quot;none&quot;) +
  theme_default()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/bias_2-1.png" width="672" />
And a large sample size (n = 200)</p>
<pre class="r"><code>bind_rows(out_0.5, out_0.65, out_0.8) %&gt;% 
  filter(n == 200) %&gt;% 
  mutate(n = str_c(&quot;n==&quot;, n),
         ICC = str_c(&quot;ICC ==&quot;, ICC)) %&gt;% 
  ggplot(aes(x = estimate, y = term)) +
  stat_dotsinterval(.width = .5, slab_shape = 22) +
  facet_wrap(vars(n, ICC), labeller = label_parsed) +
  geom_vline(aes(xintercept = 0.5)) +
  scale_slab_fill_continuous() +
  scale_slab_color_continuous() +
  coord_cartesian(ylim = c(1.4, NA)) +
  xlim(c(0,1)) +
  labs(x = &quot;Estimated regression coefficient&quot;,
       y = &quot;Method&quot;) +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = &quot;none&quot;) +
  theme_default()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/bias_3-1.png" width="672" />
In each scenario, the partial pooling hierarchical model approach produces estimates which are unbiased. The simple average method however seems to consistently underestimate the true relationship between x and y and this gets worse as the measurements get noisier (as ICC decreases).</p>
</div>
<div id="coverage" class="section level4">
<h4>Coverage</h4>
<p>Next, the coverage properties of the 95% confidence intervals are investigated to determine how often they actually cover the true regression coefficient across simulated datasets. Horizontal lines are added at 93.5% and 96.5% to cover a reasonable margin of error.</p>
<pre class="r"><code>bind_rows(out_0.5, out_0.65, out_0.8) %&gt;% 
  mutate(coverage = if_else(conf.low &lt;= 0.5 &amp; conf.high &gt;= 0.5, 1, 0)) %&gt;% 
  group_by(term, coverage, n, ICC) %&gt;% 
  tally() %&gt;% 
  mutate(percent = nn/20) %&gt;% 
  filter(coverage == 1) %&gt;%
  mutate(n = str_c(&quot;n==&quot;, n),
         n = factor(n, levels = c(&quot;n==25&quot;,&quot;n==50&quot;,&quot;n==200&quot;))) %&gt;% 
  ggplot(aes(x = n, y = percent, colour = as.factor(ICC))) +
  geom_hline(aes(yintercept = 93.5), colour = &quot;grey&quot;) +
  geom_hline(aes(yintercept = 96.5), colour = &quot;grey&quot;) +
  geom_hline(aes(yintercept = 95), colour = &quot;black&quot;) +
  geom_point(position = position_dodge(0.25)) +
  facet_wrap(~term, labeller = label_parsed) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y = &quot;Coverage %&quot;, x = &quot;&quot;, colour = &quot;ICC&quot;) +
  theme_default()</code></pre>
<pre><code>## Storing counts in `nn`, as `n` already present in input
## ℹ Use `name = &quot;new_name&quot;` to pick a new name.</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/coverage-1.png" width="672" /></p>
<p>The partial pooling approach produces estimates with approximately nominal coverage, though there is slight undercoverage in larger samples with a low ICC. On the other hand, the coverage properties of the simple averaging approach can be very poor, especially in larger samples and/or when the ICC is low.</p>
</div>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>At least in the context of this rough simulation study, using a hierarchical model to estimate the latent value of an exposure variable from replicated measurements substantially outperforms the method of simply averaging replicates. And this comes at the cost of a very slight increase in analysis complexity.</p>
<p>Importantly, both of these approaches share one problem. In each case, <em>model parameters</em> (estimates from either a linear model or hierarchical linear model) are, incorrectly, treated as if they were data. Treating them this way ignores the uncertainty associated with these parameters. Consequently, we might expect these models to produce overconfidet inferences. Nevertheless, this did not seem to have substantial consequences here, at least for the hierarchical approach. A more correct approach may be to instead estimate each of the parts of the model (estimate latent x and regress y on latent x) simultaneously in a more sophisticated hierarchical model. Such a model could be fit in Stan for instance, where each of the submodels, the model for the latent exposure and the outcome model, are fitted within the same Markov Chain.</p>
<p>Lastly, this simulation isn’t particularly realistic. In practise, measurement error may not be so well behaved and there may be other issues such as missing data. Nonetheless, I find it hard to imagine a scenario in which making full use of the sample data via a hierarchical model is not at least as good as the common averaging approach.</p>
</div>
