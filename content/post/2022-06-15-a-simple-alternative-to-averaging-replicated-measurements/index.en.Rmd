---
title: "A simple alternative to averaging replicated exposure measurements"
author: "Lachlan Cribb"
date: '2022-07-15'
slug: An-alternative-to-averaging-replicated-exposure-measurements
categories: []
tags:
- regression
- hierarchical models
subtitle: ''
summary: ''
authors: []
lastmod: '2022-07-15T19:48:43+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
header-includes: <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
---

<!-- ### Introduction -->

Exposure variables are routinely measured in replicates in many medical research fields. In my field of epidemiology, for instance, blood biomarkers are often measured in duplicate. These variables are measured in replicates because their measurement is subject to considerable technical and/or biological variability and by performing repeated measurements we can hopefully get a better picture of the latent 'true' level of the exposure.


The problem is that it is not straightforward to include replicated exposure measurements as a predictor term in a regression model. So, when one wishes to use the exposure in a regression context, it is common for the replicated measurements to be averaged first. The resulting average is then used as a predictor in subsequent regression models. This averaging smooths away some noise and outliers - giving hopefully a more accurate picture of the latent level of the exposure variable. While simple and intuitive, there is at least one problem with this approach: there is no 'borrowing of information'. That is, the data from other individuals in the study is not used in any way to inform the latent exposure level of another. 


A potential improvement on this could be to use a hierarchical model to estimate each subjects latent exposure level. With this model, we would take advantage of partial pooling to adaptively 'shrink' each participants latent exposure level towards the overall sample average, particularly if their observations are quite extreme. 

First, I briefly describe the two approaches and how they relate to eachother. 

  

#### Approach 1: simple averaging of replicates 

To see clearly how the averaging of replicates relates to the hierarchical model described later, consider the below linear model. Note that the predictions provided by this linear model are averaged replicates for each individual. In other words, this model estimates the latent exposure level for each participant simply by averaging their replicated observations.

$$
\begin{align*}
\text{x}_{ij} & \sim \mathcal N(\mu_{ij}, \sigma) \\
\mu     _{ij} & = \beta_1 \text{ID1}_{ij} + \beta_2 \text{ID2}_{ij} + \beta_3 \text{ID3}_{ij}... + \beta_n \text{IDn}_{ij}
\end{align*}
$$
Where $\text{x}_{ij}$ is the exposure measurement for the $j^{th}$ replicate for the $i^{th}$ participant, and $\mu_{it}$ represents the average exposure level for a given ID (participant). Notice that no information is shared between participants (each participant has their own intercept in the model). In other words, this model produces the so-called 'no pooling' estimate described by [Gelman and Hill](https://vulstats.ucsd.edu/pdf/Gelman.ch-12.basic-multilevel-models.pdf).

  
  

#### Approach 2: hierachical model 

In the second approach, an overall global mean (intercept) is estimated and participant-level deviations from that overall mean are represented by an error term (random intercept): 

$$
\begin{align*}
x_{ij} & \sim \mathcal N(\mu_{ij}, \sigma_e) \\
\mu     _{ij} & = \beta_0 + u_{0i} \\
u_{0i} & \sim \mathcal N(0, \sigma_0) \\
\end{align*}
$$

Alternatively, in lme4 syntax, the model is x ~ 1 + (1 | ID). 
In this model, the estimated latent exposure level for each participant is a *compromise between the overall sample average $\beta_0$ and the no pooling estimate above*. How much each estimate is pulled closer to the overall mean (how much 'partial pooling') is determined by the standard deviation of the random intercept term $\sigma_0$. As an extreme case, when $\sigma_0$ = 0, the estimate for each participant is simply the sample average. As $\sigma_0$ approaches $\infty$, the no pooling estimate is returned (resulting in identical averages to those estimated from the first model described above). Partial pooling uses the data to identify the best compromise between those extremes. 

  
  
### Simulation 

For this post, I will see how these two methods compare in a simulation study under a few different scenarios. In the simulation, the regression coefficient for the latent 'true' exposure and outcome relationship will be set to 0.50, and I will determine how well each of the two methods are able to reproduce that correct regression coefficient.

First, to demonstrate the differences between the approaches, I will create a fake dataset (n = 100) to test them both. The intraclass correlation (ICC) of the replicated exposure measurements is set to 0.65. 

```{r test, warning=FALSE, message=FALSE}

# load packages
library(tidyverse)
library(lme4)
library(broom)
library(ggdist)
library(bayesplot)


# create dataset 
d <- 
  tibble(latent_x = rnorm(100),
         y = 0.5*latent_x + rnorm(100),
         # observed x values
         x_obs1 = latent_x + rnorm(100, sd = 0.75),
         x_obs2 = latent_x + rnorm(100, sd = 0.75),
         ID = seq(1:100))
  
# approach 1: average x
d$average_x <- rowMeans(d[,3:4])
  
# approach 2: hierarchical model
  
dlong <- d %>% 
  pivot_longer(c(x_obs1,x_obs2), values_to = "x_obs")

hmod <- lmer(x_obs ~ 1 + (1 | ID), data = dlong)  
  
d$x_hmod <- predict(hmod, newdata = d)
  
```

This plot demonstrates that the estimated exposure level from the hierarchical model (x_hmod) are shrunk towards the overall sample mean, relative to the no-pooling estimate (average_x). 

```{r shrinkage_plot}

d %>% 
  slice(1:25) %>% 
  pivot_longer(c(average_x,x_hmod),
               values_to = "x", names_to = "method") %>% 
  ggplot(aes(x = x, y = ID, colour = method, group = ID)) +
  geom_point() +
  geom_vline(aes(xintercept = 0)) +
  geom_line() +
  theme_default()
```
Note that t no pooling (average_x) estimates which are quite extreme are more aggressively pooled towards the overall mean than those which relatively more mean adjacent. The hierarchical model is using the data from the whole sample and in doing so determines that those particularly extreme observations are not really to be trusted. 


To perform the simulation, I create a function to simulate data and extract the results of fitting the substantive model (y ~ x) using each approach. 

```{r sim_data}

sim_data <- function(SEED, n, noise_sd){
  
  set.seed(SEED)
  
  d <- tibble(
  latent_x = rnorm(n),
  y = 0.5*latent_x + rnorm(n),
  # observed x values
  x_obs1 = latent_x + rnorm(n, sd = noise_sd),
  x_obs2 = latent_x + rnorm(n, sd = noise_sd),
  ID = seq(1:n))
  
  d$average_x <- rowMeans(d[,3:4])
  
  # hierarchical model
  
  dlong <- d %>% 
    pivot_longer(c(x_obs1,x_obs2), values_to = "x_obs")

  hmod <- lmer(x_obs ~ 1 + (1 | ID), data = dlong)  
  
  d$x_hmod <- predict(hmod, newdata = d)
  
  results <- 
    bind_rows(est_avg = broom::tidy(lm(y ~ average_x, data = d), conf.int = T)[2,],
              est_hmod = broom::tidy(lm(y ~ x_hmod, data = d), conf.int = T)[2,])
  
  results
}
```

Now, the simulation is performed for a combination of sample sizes (25, 50, 200) and ICC (0.5, 0.65, 0.80). The latter representing quite poor, reasonable, and good reliability, respectively. 

```{r run_sims, warning=FALSE, message=FALSE}

# Perform simulation

# ICC = 0.5

out_0.5 <- 
  expand_grid(SEED = 1:2000, n = c(25, 50, 200)) %>% 
  mutate(res = map2(SEED, n, sim_data, noise_sd = 1)) %>% 
  unnest(res) %>% 
  mutate(ICC = 0.5)


# ICC = 0.65

out_0.65 <- 
  expand_grid(SEED = 1:2000, n = c(25, 50, 200)) %>% 
  mutate(res = map2(SEED, n, sim_data, noise_sd = 0.75)) %>% 
  unnest(res) %>% 
  mutate(ICC = 0.65)

# ICC = 0.8

out_0.8 <- 
  expand_grid(SEED = 1:2000, n = c(25, 50, 200)) %>% 
  mutate(res = map2(SEED, n, sim_data, noise_sd = 0.5)) %>% 
  unnest(res) %>% 
  mutate(ICC = 0.8)

```

  
  
### Results 

#### Bias 

Bias in the estimated regression coefficient for the relationship between exposure x and outcome y is presented first for small sample sizes (n = 25). 

```{r bias_1, warning=FALSE}

bind_rows(out_0.5, out_0.65, out_0.8) %>% 
  filter(n == 25) %>% 
  mutate(n = str_c("n==", n),
         ICC = str_c("ICC ==", ICC)) %>% 
  ggplot(aes(x = estimate, y = term)) +
  stat_dotsinterval(.width = .5, slab_shape = 22) +
  facet_wrap(vars(n, ICC), labeller = label_parsed) +
  geom_vline(aes(xintercept = 0.5)) +
  scale_slab_fill_continuous() +
  scale_slab_color_continuous() +
  coord_cartesian(ylim = c(1.4, NA)) +
  xlim(c(-0.5,1.5)) +
  labs(x = "Estimated regression coefficient",
       y = "Method") +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = "none") +
  theme_default()

```

Medium sample size (n = 50)

```{r bias_2, warning=FALSE}

bind_rows(out_0.5, out_0.65, out_0.8) %>% 
  filter(n == 50) %>% 
  mutate(n = str_c("n==", n),
         ICC = str_c("ICC ==", ICC)) %>% 
  ggplot(aes(x = estimate, y = term)) +
  stat_dotsinterval(.width = .5, slab_shape = 22) +
  facet_wrap(vars(n, ICC), labeller = label_parsed) +
  geom_vline(aes(xintercept = 0.5)) +
  scale_slab_fill_continuous() +
  scale_slab_color_continuous() +
  coord_cartesian(ylim = c(1.4, NA)) +
  xlim(c(0,1)) +
  labs(x = "Estimated regression coefficient",
       y = "Method") +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = "none") +
  theme_default()
```
And a large sample size (n = 200)

```{r bias_3, warning=FALSE}

bind_rows(out_0.5, out_0.65, out_0.8) %>% 
  filter(n == 200) %>% 
  mutate(n = str_c("n==", n),
         ICC = str_c("ICC ==", ICC)) %>% 
  ggplot(aes(x = estimate, y = term)) +
  stat_dotsinterval(.width = .5, slab_shape = 22) +
  facet_wrap(vars(n, ICC), labeller = label_parsed) +
  geom_vline(aes(xintercept = 0.5)) +
  scale_slab_fill_continuous() +
  scale_slab_color_continuous() +
  coord_cartesian(ylim = c(1.4, NA)) +
  xlim(c(0,1)) +
  labs(x = "Estimated regression coefficient",
       y = "Method") +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = "none") +
  theme_default()
```
In each scenario, the partial pooling hierarchical model approach produces estimates which are unbiased. The simple average method however seems to consistently underestimate the true relationship between x and y and this gets worse as the measurements get noisier (as ICC decreases). 

  
  
#### Coverage 

Next, the coverage properties of the 95% confidence intervals are investigated to determine how often they actually cover the true regression coefficient across simulated datasets. Horizontal lines are added at 93.5% and 96.5% to cover a reasonable margin of error. 

```{r coverage}

bind_rows(out_0.5, out_0.65, out_0.8) %>% 
  mutate(coverage = if_else(conf.low <= 0.5 & conf.high >= 0.5, 1, 0)) %>% 
  group_by(term, coverage, n, ICC) %>% 
  tally() %>% 
  mutate(percent = nn/20) %>% 
  filter(coverage == 1) %>%
  mutate(n = str_c("n==", n),
         n = factor(n, levels = c("n==25","n==50","n==200"))) %>% 
  ggplot(aes(x = n, y = percent, colour = as.factor(ICC))) +
  geom_hline(aes(yintercept = 93.5), colour = "grey") +
  geom_hline(aes(yintercept = 96.5), colour = "grey") +
  geom_hline(aes(yintercept = 95), colour = "black") +
  geom_point(position = position_dodge(0.25)) +
  facet_wrap(~term, labeller = label_parsed) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y = "Coverage %", x = "", colour = "ICC") +
  theme_default()

```


The partial pooling approach produces estimates with approximately nominal coverage, though there is slight undercoverage in larger samples with a low ICC. On the other hand, the coverage properties of the simple averaging approach can be very poor, especially in larger samples and/or when the ICC is low. 

  
  
### Conclusion

At least in the context of this rough simulation study, using a hierarchical model to estimate the latent value of an exposure variable from replicated measurements substantially outperforms the method of simply averaging replicates. And this comes at the cost of a very slight increase in analysis complexity. 

Importantly, both of these approaches share one problem. In each case, *model parameters* (estimates from either a linear model or hierarchical linear model) are, incorrectly, treated as if they were data. Treating them this way ignores the uncertainty associated with these parameters. Consequently, we might expect these models to produce overconfident inferences. Nevertheless, this did not seem to have substantial consequences here, at least for the hierarchical approach. A more correct approach may be to instead estimate each of the parts of the model (estimate latent x and regress y on latent x) simultaneously in a more sophisticated hierarchical model. Such a model could be fit in Stan for instance, where each of the submodels, the model for the latent exposure and the outcome model, are fitted within the same Markov Chain. 

Lastly, this simulation isn't particularly realistic. In practise, measurement error may not be so well behaved and there may be other issues such as missing data. Nonetheless, I find it hard to imagine a scenario in which making full use of the sample data via a hierarchical model is not at least as good as the more common averaging approach. 
