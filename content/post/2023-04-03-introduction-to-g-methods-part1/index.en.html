---
title: "Introduction to g-methods: time fixed treatments"
author: "Lachlan Cribb"
date: '2023-04-03'
slug: Introduction-to-g-methods-time-fixed
categories: []
tags:
- causal inference
- marginal structural models
subtitle: ''
summary: ''
authors: []
lastmod: '2023-04-03T19:48:43+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="potential-outcomes" class="section level3">
<h3>Potential outcomes</h3>
<p>The goal of causally motivated epidemiology research is often to estimate the average treatment effect (ATE). For a binary treatment or exposure <span class="math inline">\(A\)</span> the ATE on the difference scale is defined like so:
<span class="math display">\[ E[Y_i^{a=1} - Y_i^{a=0}]\]</span></p>
<p>Where <span class="math inline">\(Y_i^{a=1}\)</span> denotes the value of the outcome variable when treatment is given for participant <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i^{a=0}\)</span> denotes the value of the outcome variable when treatment is not given, respectively (from here on out, the <span class="math inline">\(i\)</span> subscript will be treated as implicit). As participants in a study only receive one of the two treatments on offer (they are either treated or untreated), only one these outcomes is actually observed for a given individual. For this reason, the outcomes <span class="math inline">\(Y^{a=1}\)</span> and <span class="math inline">\(Y^{a=0}\)</span> are often referred to as <em>potential</em> or <em>counterfactual</em> outcomes as they represent the value of the outcome variable in an individual had they, potentially counter-to-fact, received the treatment level <span class="math inline">\(a\)</span>.</p>
<p>The expression above quantifies the average difference between the two potential outcomes. Put another way, it quantifies the average difference in the level of <span class="math inline">\(Y\)</span> in a hypothetical world in which everyone been treated (<span class="math inline">\(a=1\)</span>) and the level of <span class="math inline">\(Y\)</span> in a hypothetical world in which no-one had been treated (<span class="math inline">\(a=0\)</span>), all else being equal. It is the estimated effect of moving the entire population from untreated to treated. (Note that the subtraction in the expression above could be replaced by division, in which case the average treatment effect would then be on the multiplicative scale, e.g. a causal risk ratio for a binary outcome).</p>
<p>Given that the expression above deals with quantities which are by their very nature unobservable, an obvious question is then how can we estimate this treatment effect with real data? To do so, a few (mostly untestable) assumptions are required.</p>
</div>
<div id="identification-assumptions" class="section level3">
<h3>Identification assumptions</h3>
<p>The three core assumptions or conditions required to link observed data with counterfactual/potential outcomes are as follows:</p>
<ul>
<li>Consistency (a.k.a well-defined intervention assumption): The potential outcome for an individual is equal to their observed outcome for the treatment they actually received. Formally, <span class="math inline">\(Y^a = Y\)</span> for individuals with <span class="math inline">\(A = a\)</span>. What this means is that interventions must be sufficiently well-defined in order for potential outcomes <span class="math inline">\(Y^a\)</span> to be well-defined.</li>
<li>(Conditional) exhangeability: Within levels of the measured confounding variables <span class="math inline">\(L\)</span>, treated and untreated groups are exchageable. In other words, treatment is effectively randomly assigned within levels of the measured confounding variables.</li>
<li>Positivity: For all individuals, there is some (positive) chance of receiving each of the treatment levels. Formally, <span class="math inline">\(Pr[A=a|L=l] &gt; 0\)</span> for all <span class="math inline">\(Pr[L=l] &gt; 0\)</span>.</li>
</ul>
<p>When these three essential conditions are met (or approximately met) for an observational study, we may then estimate causal effects with the data we have at hand. One set of methods to do so are referred to as the <em>g-methods</em>, a set of methods for estimating treatment effects in the context of time fixed or time-varying treatments (g-methods are in fact almost essential for the latter).</p>
<p>Ignoring a third method (g-estimation) which is not particularly well supported by popular software, the g-methods consists of two main strategies for estimating treatment effects: the g-formula and inverse probability of treatment weighting (IPTW) of marginal structural models. Though both of these methods rely on the same set of identification conditions (those described above), they make a different set of modelling assumptions. Each method will now be described with an applied example of a time-fixed treatment.</p>
</div>
<div id="the-g-formula" class="section level3">
<h3>The g-formula</h3>
<p>Assuming exchangeability, positivity and consistency, the g-formula for a time fixed treatment is:
<span class="math display">\[
E[Y^a] = \sum_l E[Y|A=a,L=l]Pr[L=l]
\]</span></p>
<p>Here <span class="math inline">\(E[Y^a]\)</span> is the counterfactual mean of <span class="math inline">\(Y\)</span> with treatment set at <span class="math inline">\(a\)</span>. <span class="math inline">\(L\)</span> is a vector of confounding variables (e.g., sex, education). The sum indicates that we are taking a weighted average of the conditional mean of <span class="math inline">\(Y\)</span> where weights are the prevalence of each value <span class="math inline">\(l\)</span> in the population. In other words, we are computing the mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(L\)</span> and then averaging over the distribution of <span class="math inline">\(L\)</span>. The resulting counterfactual quantity <span class="math inline">\(E[Y^a]\)</span> is a marginal one, in that it is not conditional on confounding variables <span class="math inline">\(L\)</span>.</p>
<p>The above equation only works when the variables within L are discrete, otherwise the sum becomes an integral:</p>
<p><span class="math display">\[
E[Y^a] = \int E[Y|A=a,L=l] dF_L[L=l]
\]</span>
Where <span class="math inline">\(F_L\)</span> is the joint cumulative distribution function of the random variables in <span class="math inline">\(L\)</span>. This is going to be hard to deal with, especially when multiple confounders are present.</p>
<p>Thankfully, we do not need to go the effort of obtaining <span class="math inline">\(Pr[L=l]\)</span> or <span class="math inline">\(F_L[L=l]\)</span>. What we can do instead is estimate <span class="math inline">\(E[Y|A=a,L=l]\)</span> for the particular <span class="math inline">\(l\)</span> for each individual in the study and then compute the average:
<span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n}\hat{E}[Y|A=a, L_i]
\]</span></p>
<p>In effect, we are averaging over the distribution of confounders <span class="math inline">\(L\)</span> in the sample (this process is also known as <em>standardisation</em>). The computational means for doing this are straightforward. It proceeds like so:</p>
<ol style="list-style-type: decimal">
<li>Fit a model (e.g., linear or logistic regression) for the conditional mean of the outcome given treatment and confounders</li>
<li>Create two copies of the dataset. In the first, set treatment to 0 (untreated) and in the second, set treatment to 1 (treated)</li>
<li>Calculate predicted values <span class="math inline">\(\hat{E}[Y|A=a,L=l]\)</span> from the fitted model for each of the two artificial datasets</li>
<li>Average each set of predicted values to get <span class="math inline">\(E[Y^{a=1}]\)</span> and <span class="math inline">\(E[Y^{a=0}]\)</span></li>
<li>Contrast the predicted values. E.g., <span class="math inline">\(E[Y^{a=1}] - E[Y^{a=0}]\)</span> or <span class="math inline">\(\frac{E[Y^{a=1}]}{E[Y^{a=0}]}\)</span></li>
</ol>
<p>When a parametric model (such as a linear model) is used to estimate the conditional mean of the outcome given treatment and the confounders, this method is referred to as the <em>parametric g-formula</em>.</p>
<div id="applying-the-g-formula" class="section level4">
<h4>Applying the g-formula</h4>
<p>The NHEFS dataset available <a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/">here</a>, used by Hernan and Robins in their book Causal Inference, will be used as an example. The goal of this analysis is to estimate the effect of smoking cessation between baseline (1971) and follow-up (1982) on weight gain during the same interval. First, we’ll load the data and fit a linear regression model (step 1). We fit the same model as Hernan and Robins, including confounding variables sex, race, age, education, smoking intensity, smoking years, exercise, activeness, and baseline weight. Departures from linearity are allowed for continuous variables by including a quadratic term with the rms pol() function.</p>
<pre class="r"><code>library(here)
library(tidyverse)
library(rms)

nhefs &lt;- read_csv(&quot;nhefs.csv&quot;)

# remove participants missing outcome data 

nhefs &lt;- nhefs |&gt; filter(!is.na(wt82_71))

# set type for factor variables                  
nhefs &lt;- nhefs |&gt; mutate(across(c(education,exercise,active), as.factor))

# fit model                  
fit &lt;-
  lm(wt82_71 ~ qsmk + sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) +       exercise + active + pol(wt71) + qsmk:smokeintensity, data = nhefs)</code></pre>
<p>For step 2, we create two artificial copies of the dataset with treatment (qsmk) set to 0 and 1, respectively.</p>
<pre class="r"><code>nhefs.0 &lt;- nhefs.1 &lt;- nhefs

nhefs.0$qsmk &lt;- 0

nhefs.1$qsmk &lt;- 1</code></pre>
<p>For the final two steps, we calculate predictions for each of the artificial datasets, take their average, and contrast them.</p>
<pre class="r"><code>fitted.0 &lt;- predict(fit, newdata = nhefs.0)
fitted.1 &lt;- predict(fit, newdata = nhefs.1)

E_1 &lt;- mean(fitted.1)
E_0 &lt;- mean(fitted.0)

treatment_effect &lt;- E_1 - E_0

print(paste(&quot;E[Y(1)] - E[Y(0)] = &quot;,round(treatment_effect,2)))</code></pre>
<pre><code>## [1] &quot;E[Y(1)] - E[Y(0)] =  3.52&quot;</code></pre>
<p>The estimated effect of smoking cessation, relative to no cessation, is a weight gain of 3.52 kg.</p>
</div>
<div id="standard-errors-for-g-formula-estimates" class="section level4">
<h4>Standard errors for g-formula estimates</h4>
<p>There are several ways to obtain confidence intervals for a g-formula estimate. One relatively straightforward option is bootstrapping.</p>
<pre class="r"><code>library(boot)

parametric_g &lt;- function(data, indices){
  
  df &lt;- nhefs[indices,]
  
  fit &lt;-
  lm(wt82_71 ~ qsmk + sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) +         exercise + active + pol(wt71) + qsmk:smokeintensity, data = df)
  
  df.0 &lt;- df.1 &lt;- df
  df.0$qsmk &lt;- 0
  df.1$qsmk &lt;- 1
  
  fitted.0 &lt;- predict(fit, newdata = df.0)
  fitted.1 &lt;- predict(fit, newdata = df.1)
  
  E_1 &lt;- mean(fitted.1)
  E_0 &lt;- mean(fitted.0)
  
  treatment_effect &lt;- E_1 - E_0

  return(treatment_effect)
}

boot_out &lt;- boot(data = nhefs,
                 statistic = parametric_g,
                 R = 100)

boot.ci(boot_out, conf = 0.95, type = &quot;norm&quot;)</code></pre>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 100 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot_out, conf = 0.95, type = &quot;norm&quot;)
## 
## Intervals : 
## Level      Normal        
## 95%   ( 2.586,  4.385 )  
## Calculations and Intervals on Original Scale</code></pre>
<p>The bootstrap 95% confidence interval ranges from 2.6 to 4.5.</p>
<p>There are other, less computationally costly, means of obtaining confidence intervals - the delta method and simulation. We will check how well the above bootstrap intervals agree with those obtained with the delta method using R package <a href="https://vincentarelbundock.github.io/marginaleffects/">marginaleffects</a> and with those obtained by simulation using R package <a href="https://iqss.github.io/clarify/">clarify</a>.</p>
<p>First, the delta method:</p>
<pre class="r"><code>library(marginaleffects)

## First, delta method

avg_comparisons(fit, variables = list(qsmk = 0:1))</code></pre>
<pre><code>## 
##  Term Contrast Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %
##  qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001  2.65   4.38
## 
## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high</code></pre>
<p>Next, by simulation:</p>
<pre class="r"><code>library(clarify)

sim_coefs &lt;- sim(fit)

sim_est &lt;- sim_ame(sim_coefs, var = &quot;qsmk&quot;, contrast = &quot;diff&quot;, verbose = FALSE)

summary(sim_est)</code></pre>
<pre><code>##         Estimate 2.5 % 97.5 %
## E[Y(0)]     1.76  1.34   2.17
## E[Y(1)]     5.27  4.55   6.01
## Diff        3.52  2.59   4.44</code></pre>
<p>The confidence intervals produced by all three methods are similar.</p>
<p>Beyond the core identification conditions described above, there is another essential assumption for the parametric g-formula to be unbiased. That is that the conditional mean outcome model is correctly specified. Even if you have meausured all important confounders you may not get an unbiased causal effect estimate from this method if you have misspecified the model (e.g., missed important interactions or non-linearities). The method described in the next section relies on a different modelling assumption - that is, that the model for <em>treatment</em> is correctly specified.</p>
</div>
</div>
<div id="ipt-weighting" class="section level3">
<h3>IPT weighting</h3>
<p>The aim of inverse probability of treatment (IPT) weighting is to create a <em>pseudo-population</em> in which treatment <span class="math inline">\(A\)</span> and confounders <span class="math inline">\(L\)</span> are statistically independent. Provided identification conditions are satisfied, <span class="math inline">\(E[Y^a]\)</span> in the actual population is equal to <span class="math inline">\(E_{ps}[Y|A=a]\)</span> in the pseudo-population. The pseudo-population is created by weighting each individual by the inverse of the probability of receiving the treatment that they received. Formally, the weights are defined as:
<span class="math display">\[
W^A = \frac{1}{f(A|L)}
\]</span></p>
<p>These weights can be improved upon by using the (sometimes) more statistically efficient stabilised IP weights. The formula is:
<span class="math display">\[
W^A = \frac{f(A)}{f(A|L)}
\]</span></p>
<p>Stabilised weights are preferable to unstabilised weights for the additional reason that they are useful more generally (e.g., when exposure is a continuous variable, in which case unstabilised weights are unviable).</p>
<p>Now we will again estimate the causal effect of smoking cessation on weight gain, this time using IPT weighting rather than the parametric g-formula. We will use stabilised IPT weights. The steps are as follows:</p>
<ol style="list-style-type: decimal">
<li>Use logistic regression to estimate the numerator of the weights. I.e., the marginal probability of treatment</li>
<li>Use logistic regression to estimate the denominator of the weights. I.e., the coditional probability of treatment given confounders <span class="math inline">\(L\)</span></li>
<li>Estimate <span class="math inline">\(W^A\)</span> using the formula above</li>
<li>Lastly, fit a weighted regression model to estimate the treatment effect. Because IP weighting must be taken into account for standard errors to be correct, we use a method which provides robust ‘sandwhich’ type standard errors (bootstrapping is an alternative option).</li>
</ol>
<p>Provided identifiability conditions are satisfied, the model fitted to the pseudo-population created in step 4 has the form: <span class="math display">\[E[Y^a]=\beta_0 + \beta_1 a\]</span>
This is referred to as a <em>marginal structural model</em>. It is marginal because the outcome is a marginal quantity &amp; structural as it is a model for a <em>counterfactual</em>, rather than fully observed, outcome.</p>
<div id="applying-ipt-weighting" class="section level4">
<h4>Applying IPT weighting</h4>
<p>The code below performs the four steps above:</p>
<pre class="r"><code>library(geepack) # for robust standard errors

# fit model for numerator (step 1)

fit_num &lt;- glm(qsmk ~ 1, data = nhefs, family = binomial())

numerator &lt;- ifelse(nhefs$qsmk == 1, 
                    predict(fit_num, type = &quot;response&quot;),
                    1 - predict(fit_num, type = &quot;response&quot;))

# fit model for denominator (step 2)

fit_denom &lt;- glm(qsmk ~ sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) + exercise + active + pol(wt71), 
            family = binomial(), data = nhefs)

denominator &lt;- ifelse(nhefs$qsmk == 1,
                      predict(fit_denom, type = &quot;response&quot;),
                      1 - predict(fit_denom, type = &quot;response&quot;))

# calculate weights (setp 3)

ipt_weights &lt;- numerator / denominator 

# fit marginal structural model (step 4)

ip_mod &lt;- geeglm(wt82_71 ~ qsmk, data = nhefs, weights = ipt_weights,
                 id = seqn, corstr = &quot;independence&quot;)

# inference 

beta &lt;- round(coef(ip_mod),2)
SE &lt;- coef(summary(ip_mod))[, 2]
lcl &lt;- round(beta - 1.96 * SE,2)
ucl &lt;- round(beta + 1.96 * SE,2)
cbind(beta, lcl, ucl)</code></pre>
<pre><code>##             beta  lcl  ucl
## (Intercept) 1.78 1.34 2.22
## qsmk        3.44 2.41 4.47</code></pre>
<p>The IPTW estimate for smoking cessation is a 3.44 kg increase in weight gain, with a 95% CI ranging from 2.4 to 4.5. Very similar to that obtained by the parametric g-formula. This is a reassuring sign as these two methods rely on different modeling assumptions, as will be touched on later.</p>
</div>
<div id="checking-balance" class="section level4">
<h4>Checking balance</h4>
<p>The central idea between IPT weighting is that, in the pseudo-population produced by the weights, treatment and confounders are statistically independent. As discussed in the paper by <a href="https://onlinelibrary.wiley.com/doi/10.1002/sim.6607">Austin &amp; Stuart (2015)</a>, it is best practise to assess that the estimated weights are working as they should and that measured confounders are balanced across levels of treatment. This can be easily checked using The bal.tab function of the <a href="%22https://ngreifer.github.io/cobalt/index.html%22">cobalt</a> package. In our case, these results (not shown) indicate that measured confounders are well balanced across treatment in the pseudo-population.</p>
</div>
</div>
<div id="doubly-robust-estimation" class="section level3">
<h3>Doubly robust estimation</h3>
<p>As mentioned previously, key assumptions of the parametric g-formula and IPTW estimators are that the models for the outcome and treatment, respectively, are correctly specified. Doubly robust (DR) estimators, on the other hand, combine models for both treatment and the outcome and by doing so, give the analyst <em>two chances</em> to get the model right. That is, a DR estimator will be consistent so long as <em>at least one</em> of the outcome or treatment models are correctly specified. And, as a bonus, if both models are correct, the DR estimator will have a smaller variance than the IPTW estimator.</p>
<p>We will use the DR estimator of <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x">Bang and Robins (2005)</a> as an example. The steps are as follows:</p>
<ol style="list-style-type: decimal">
<li>Estimate the probability of treatment <span class="math inline">\(A\)</span> given confounders <span class="math inline">\(L\)</span> using a logistic regression model</li>
<li>Compute the ‘clever covariate’ <span class="math inline">\(R\)</span> which equals <span class="math inline">\(1/Pr[A=1|L=l]\)</span> when treatment is given and <span class="math inline">\(1/-(1-Pr[A|L=l])\)</span> when treatment is not given</li>
<li>Fit a regression model for the conditional mean of the outcome, including clever covariate <span class="math inline">\(R\)</span> as a predictor</li>
<li>Standardisation as per the g-formula method above. Note, in the artificial dataset where treatment is given (<span class="math inline">\(A=1\)</span>), the clever covariate <span class="math inline">\(R\)</span> should take value <span class="math inline">\(1/Pr[A=1|L=l]\)</span>. In the artificial dataset where treatment is not given (<span class="math inline">\(A=0\)</span>), <span class="math inline">\(R\)</span> takes value <span class="math inline">\(1/-(1-Pr[A|L=l])\)</span></li>
</ol>
<div id="applying-the-doubly-robust-estimator" class="section level4">
<h4>Applying the doubly robust estimator</h4>
<p>And here is the code to apply the estimator to the smoking cessation data (thank you to Noah Greifer for finding and correcting an earlier <a href="https://stats.stackexchange.com/questions/615519/bang-and-robins-doubly-robust-estimator-biased-and-with-large-variance/615583?noredirect=1#comment1143991_615583">error</a> in my code):</p>
<pre class="r"><code># fit model for treatment

treatment_mod &lt;- glm(qsmk ~ sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) + exercise + active + pol(wt71), 
                       family = binomial(), data = nhefs)

prob_treatment &lt;- predict(treatment_mod, type = &quot;response&quot;)

# Create &#39;clever covariate&#39; 

R &lt;- ifelse(nhefs$qsmk == 1,
            1 / prob_treatment,
            1 / -(1 - prob_treatment))

# Fit outcome model including clever covariate 

outcome_mod &lt;- lm(wt82_71 ~ qsmk + sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) + exercise + active + pol(wt71) + qsmk:smokeintensity + R, data = nhefs)

# standardise 

nhefs.0 &lt;- nhefs.1 &lt;- nhefs

nhefs.0$qsmk &lt;- 0
nhefs.0$R &lt;- 1 / -(1 - prob_treatment)

nhefs.1$qsmk &lt;- 1
nhefs.1$R &lt;- 1 / prob_treatment

E_0 &lt;- mean(predict(outcome_mod, newdata = nhefs.0))

E_1 &lt;- mean(predict(outcome_mod, newdata = nhefs.1))

treat.effect &lt;- E_1 - E_0

print(paste(&quot;E[Y(1)] - E[Y(0)] = &quot;,round(treat.effect,2)))</code></pre>
<pre><code>## [1] &quot;E[Y(1)] - E[Y(0)] =  3.45&quot;</code></pre>
<p>The DR estimate is very similar to those of the g-formula and IPTW estimators. Confidence intervals could be obtained by bootstrapping.</p>
</div>
</div>
<div id="targeted-maximum-likelihood-estimation-tmle" class="section level3">
<h3>Targeted maximum likelihood estimation (TMLE)</h3>
<p>All of the above approaches for the estimation of causal effects have made use of standard parametric models for estimation (that is, linear and logistic regression). Because these models tend to impose restrictions on the manner in which covariates are related to the outcome (e.g., no or few interactions, linearity of continuous variables), some amount of model mispecification is very likely in real settings. By contrast, highly flexible machine learning models impose few such restrictions and may be less prone to misspecification related bias.</p>
<p>TMLE is a procedure for the doubly robust estimation of causal effects which makes use of machine learning tools. The basic mechanics, as outlined by <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7628">Luque-Fernandez et al (2018)</a>, are similar to that of the Bang and Robins estimator, though a little more involved. The full procedure will be explored in more detail in a later post. For now, we’ll go straight to estimating the effect of smoking cessation using the R tmle package.</p>
<p>Models for the conditional probability of treatment and the conditional mean on the outcome, given confounders <span class="math inline">\(L\)</span>, will be estimated using <a href="https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html#background">SuperLearner</a>. SuperLearner is an algorithm which uses several machine learning tools, estimates their performance using cross-validation, and then creates an optimally weighted average of those models (an “ensemble”). Here, we’ll make use of generalised linear models, penalised maximum likelihood (glmnet), generalised additive models, and XGBoost in the SuperLearner algorithm.</p>
<pre class="r"><code>library(SuperLearner)</code></pre>
<pre><code>## Loading required package: nnls</code></pre>
<pre><code>## Loading required package: gam</code></pre>
<pre><code>## Loading required package: splines</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loaded gam 1.22-2</code></pre>
<pre><code>## Super Learner</code></pre>
<pre><code>## Version: 2.0-28</code></pre>
<pre><code>## Package created on 2021-05-04</code></pre>
<pre class="r"><code>library(tmle)</code></pre>
<pre><code>## Loading required package: glmnet</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loaded glmnet 4.1-7</code></pre>
<pre><code>## Welcome to the tmle package, version 1.5.0-1.1
## 
## Major changes since v1.3.x. Use tmleNews() to see details on changes and bug fixes</code></pre>
<pre class="r"><code># data.frame of confounder variables

confounds &lt;- select(nhefs, sex, race, age, education, smokeintensity, smokeyrs, exercise, active, wt71)

SL.library = c(&quot;SL.glm&quot;, 
               &quot;SL.glmnet&quot;,
               &quot;SL.gam&quot;,
               &quot;SL.xgboost&quot;) 

tmle.out &lt;- tmle(Y = nhefs$wt82_71, 
                 A = nhefs$qsmk, 
                 W = confounds, 
                 family = &quot;gaussian&quot;, 
                 V = 5,
                 Q.SL.library = SL.library, 
                 g.SL.library = SL.library)</code></pre>
<pre><code>## Loading required namespace: xgboost</code></pre>
<pre class="r"><code>tmle.out</code></pre>
<pre><code>##  Additive Effect
##    Parameter Estimate:  3.5184
##    Estimated Variance:  0.2306
##               p-value:  2.3571e-13
##     95% Conf Interval: (2.5772, 4.4596) 
## 
##  Additive Effect among the Treated
##    Parameter Estimate:  3.4367
##    Estimated Variance:  0.22192
##               p-value:  2.9795e-13
##     95% Conf Interval: (2.5134, 4.3601) 
## 
##  Additive Effect among the Controls
##    Parameter Estimate:  3.546
##    Estimated Variance:  0.24454
##               p-value:  7.4607e-13
##     95% Conf Interval: (2.5768, 4.5153)</code></pre>
<p>The estimate and confidence interval are very similar to those from previous methods. Which might indicate that our parameteric models were reasonably well-specified, though it remains uncertain whether other conditions (such as no unmeasured confounding) are satisfied.</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>Hernán, M. A., &amp; Robins, J. M. (2010). Causal inference.</p>
<p>Arel-Bundock V (2023). marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests. R package version 0.9.0, <a href="https://vincentarelbundock.github.io/marginaleffects/" class="uri">https://vincentarelbundock.github.io/marginaleffects/</a>.</p>
<p>Greifer N, Worthington S, Iacus S, King G (2023). clarify: Simulation-Based Inference for Regression Models. <a href="https://github.com/iqss/clarify" class="uri">https://github.com/iqss/clarify</a>, <a href="https://iqss.github.io/clarify/" class="uri">https://iqss.github.io/clarify/</a>.</p>
<p>Austin, P. C., &amp; Stuart, E. A. (2015). Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies. Statistics in medicine, 34(28), 3661-3679.</p>
<p>Greifer N (2023). cobalt: Covariate Balance Tables and Plots. <a href="https://ngreifer.github.io/cobalt/" class="uri">https://ngreifer.github.io/cobalt/</a>, <a href="https://github.com/ngreifer/cobalt" class="uri">https://github.com/ngreifer/cobalt</a>.</p>
<p>Bang, H., &amp; Robins, J. M. (2005). Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4), 962-973.</p>
<p>Luque‐Fernandez, M. A., Schomaker, M., Rachet, B., &amp; Schnitzer, M. E. (2018). Targeted maximum likelihood estimation for a binary treatment: A tutorial. Statistics in medicine, 37(16), 2530-2546.</p>
</div>
