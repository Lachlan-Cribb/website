---
title: "Introduction to g-methods: time fixed treatments"
author: "Lachlan Cribb"
date: '2023-04-03'
slug: Introduction-to-g-methods-time-fixed
categories: []
tags:
- causal inference
- marginal structural models
subtitle: ''
summary: ''
authors: []
lastmod: '2023-04-03T19:48:43+10:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

### Potential outcomes ###

The goal of causally motivated epidemiology research is often to estimate the average treatment effect (ATE). For a binary treatment or exposure $A$ the ATE on the difference scale is defined like so: 
$$ E[Y_i^{a=1} - Y_i^{a=0}]$$

Where $Y_i^{a=1}$ denotes the value of the outcome variable when treatment is given for participant $i$ and $Y_i^{a=0}$ denotes the value of the outcome variable when treatment is not given, respectively (from here on, the $i$ subscript will be implicit). As participants in a study only receive one of the two treatments on offer (they are either treated or untreated), only one these outcomes is actually observed for a given individual. For this reason, the outcomes $Y^{a=1}$ and $Y^{a=0}$ are often referred to as *potential* or *counterfactual* outcomes as they represent the value of the outcome variable in an individual had they, potentially counter-to-fact, received the treatment level $a$. 

The expression above quantifies the average difference between the two potential outcomes. Put another way, it quantifies the average difference in the level of $Y$ in a hypothetical world in which everyone been treated ($a=1$) and the level of $Y$ in a hypothetical world in which no-one had been treated ($a=0$), all else being equal. It is the estimated effect of moving the entire population from untreated to treated.

Given that the expression above deals with quantities which are at best only partially observed, an obvious question is then how can we estimate this treatment effect with real data? To do so, a few (mostly untestable) assumptions are required. 


### Identification assumptions ###

The three core assumptions or conditions required to link observed data with counterfactual/potential outcomes are as follows: 

###### *Consistency* ######

Consistency (a.k.a well-defined intervention assumption) is the assumption that the potential outcome for an individual is equal to their observed outcome for the treatment they actually received. Formally, $Y^a = Y$ for individuals with $A = a$. This means that interventions must be sufficiently well-defined in order for potential outcomes $Y^a$ to be well-defined.

###### *(Conditional) exhangeability* ######

Conditional exchangeability implies that, within levels of the measured confounding variables $L$, treated and untreated groups are exchageable. In other words, treatment is effectively randomly assigned within levels of the measured confounding variables.

###### *Positivity* ######

For all individuals, there is some (positive) chance of receiving each of the treatment levels. Formally, $Pr[A=a|L=l] > 0$ for all $Pr[L=l] > 0$. 


When these three essential conditions are met (or approximately met) for an observational study, we may then estimate causal effects with the data we have at hand. One set of methods to do so are referred to as the *g-methods*, a set of methods for estimating treatment effects in the context of time fixed or time-varying treatments. 

Ignoring a third method (g-estimation) which is not particularly well supported by popular software, the g-methods consists of two main strategies for estimating treatment effects: the g-formula and inverse probability of treatment weighting (IPTW) of marginal structural models. Each method will now be described with an applied example of a time-fixed treatment. 

### The g-formula ###

Assuming exchangeability, positivity and consistency, the g-formula for a time fixed treatment is:
$$
E[Y^a] = \sum_l E[Y|A=a,L=l]Pr[L=l]
$$

Here $E[Y^a]$ is the counterfactual mean of $Y$ with treatment set at $a$. $L$ is a vector of confounding variables (e.g., sex, education). The sum indicates that we are taking a weighted average of the conditional mean of $Y$ where weights are the prevalence of each value $l$ in the population. In other words, we are computing the mean of $Y$ given $L$ and then averaging over the distribution of $L$. The resulting counterfactual quantity $E[Y^a]$ is a marginal one, in that it is not conditional on confounding variables $L$. 

The above equation only works when the variables within L are discrete, otherwise the sum becomes an integral:

$$
E[Y^a] = \int E[Y|A=a,L=l] dF_L[L=l]
$$
Where $F_L$ is the joint cumulative distribution function of the random variables in $L$. This is going to be hard to deal with, especially when multiple confounders are present. 

Thankfully, we do not need to go the effort of obtaining $Pr[L=l]$ or $F_L[L=l]$. What we can do instead is estimate $E[Y|A=a,L=l]$ for the particular $l$ for each individual in the study and then compute the average: 
$$
\frac{1}{n}\sum_{i=1}^{n}\hat{E}[Y|A=a, L_i]
$$

In effect, we are averaging over the distribution of confounders $L$ in the sample (this process is also known as *standardisation*). The computational means for doing this are straightforward. 

It proceeds like so:

1) Fit a model (e.g., linear or logistic regression) for the conditional mean of the outcome given treatment and confounders
2) Create two copies of the dataset. In the first, set treatment to 0 (untreated) and in the second, set treatment to 1 (treated)
3) Calculate predicted values $\hat{E}[Y|A=a,L=l]$ from the fitted model for each of the two artificial datasets
4) Average each set of predicted values to get $E[Y^{a=1}]$ and $E[Y^{a=0}]$
5) Contrast the predicted values. E.g., $E[Y^{a=1}] - E[Y^{a=0}]$ or $\frac{E[Y^{a=1}]}{E[Y^{a=0}]}$

When a parametric model (such as a linear model) is used to estimate the conditional mean of the outcome given treatment and the confounders, this method is referred to as the *parametric g-formula*. 

#### Applying the g-formula ####

The NHEFS dataset available [here](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/), used by Hernan and Robins in their book Causal Inference, will be used as an example. The goal of this analysis is to estimate the effect of smoking cessation between baseline (1971) and follow-up (1982) on weight gain during the same interval. First, we'll load the data and fit a linear regression model (step 1). We fit the same model as Hernan and Robins, including confounding variables sex, race, age, education, smoking intensity, smoking years, exercise, activeness, and baseline weight. Departures from linearity are allowed for continuous variables by including a quadratic term with the rms pol() function. 

```{r load NHEFS, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(rms)

nhefs <- read_csv("nhefs.csv")

# remove participants missing outcome data 

nhefs <- nhefs |> filter(!is.na(wt82_71))

# set type for factor variables                  
nhefs <- nhefs |> mutate(across(c(education,exercise,active), as.factor))

# fit model                  
fit <-
  lm(wt82_71 ~ qsmk + sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) +       exercise + active + pol(wt71) + qsmk:smokeintensity, data = nhefs)
```

For step 2, we create two artificial copies of the dataset with treatment (qsmk) set to 0 and 1, respectively.

```{r artificial data}

nhefs.0 <- nhefs.1 <- nhefs

nhefs.0$qsmk <- 0

nhefs.1$qsmk <- 1
```

For the final three steps, we calculate predictions for each of the artificial datasets, take their average, and contrast them. 

```{r standardisation effect}

fitted.0 <- predict(fit, newdata = nhefs.0)
fitted.1 <- predict(fit, newdata = nhefs.1)

E_1 <- mean(fitted.1)
E_0 <- mean(fitted.0)

treatment_effect <- E_1 - E_0

print(paste("E[Y(1)] - E[Y(0)] = ",round(treatment_effect,2)))

```

The estimated effect of smoking cessation, relative to no cessation, is a weight gain of 3.52 kg. 


#### Confidence intervals for g-formula estimates ####

There are several ways to obtain confidence intervals for a g-formula estimate. One relatively straightforward option is bootstrapping.

```{r boostrapping standardisation, warning=FALSE, message=FALSE}

library(boot)

parametric_g <- function(data, indices){
  
  df <- nhefs[indices,]
  
  fit <-
  lm(wt82_71 ~ qsmk + sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) +         exercise + active + pol(wt71) + qsmk:smokeintensity, data = df)
  
  df.0 <- df.1 <- df
  df.0$qsmk <- 0
  df.1$qsmk <- 1
  
  fitted.0 <- predict(fit, newdata = df.0)
  fitted.1 <- predict(fit, newdata = df.1)
  
  E_1 <- mean(fitted.1)
  E_0 <- mean(fitted.0)
  
  treatment_effect <- E_1 - E_0

  return(treatment_effect)
}

boot_out <- boot(data = nhefs,
                 statistic = parametric_g,
                 R = 100)

boot.ci(boot_out, conf = 0.95, type = "norm")
```

The bootstrap 95% confidence interval ranges from 2.6 to 4.5. 

There are other, less computationally costly, means of obtaining confidence intervals - the delta method and simulation. 

###### *Delta method* ######
THe R package [marginaleffects](https://vincentarelbundock.github.io/marginaleffects/) applies the parametric g-formula and computes standard errors using the delta method: 

```{r delta method}
library(marginaleffects)

## First, delta method

avg_comparisons(fit, variables = list(qsmk = 0:1))
```

###### *Simulation* ######

Confidence intervals for the average treatment effect can also be computed by simulation using R package [clarify](https://iqss.github.io/clarify/). 

```{r simulated intervals}
library(clarify)

sim_coefs <- sim(fit)

sim_est <- sim_ame(sim_coefs, var = "qsmk", contrast = "diff", verbose = FALSE)

summary(sim_est)
```

The confidence intervals produced by all three methods are similar. 

Beyond the core identification conditions described above, there is another essential assumption for the parametric g-formula to be unbiased. That is that the conditional mean outcome model is correctly specified. Even if you have meausured all important confounders you may not get an unbiased causal effect estimate from this method if you have misspecified the model (e.g., missed important interactions or non-linearities). The method described in the next section relies on a different modelling assumption - that is, that the model for *treatment* is correctly specified. 


### IPT weighting ###

The aim of inverse probability of treatment (IPT) weighting is to create a *pseudo-population* in which treatment $A$ and confounders $L$ are statistically independent. Provided identification conditions are satisfied, $E[Y^a]$ in the actual population is equal to $E_{ps}[Y|A=a]$ in the pseudo-population. The pseudo-population is created by weighting each individual by the inverse of the probability of receiving the treatment that they received. Formally, the weights are defined as:
$$ 
W^A = \frac{1}{f(A|L)}
$$

These weights can be improved upon by using the (sometimes) more statistically efficient stabilised IP weights. The formula is: 
$$
W^A = \frac{f(A)}{f(A|L)}
$$

Now we will again estimate the causal effect of smoking cessation on weight gain, this time using IPT weighting rather than the parametric g-formula. We will use stabilised IPT weights.

The steps are as follows:

1) Use logistic regression to estimate the numerator of the weights. I.e., the marginal probability of treatment 
2) Use logistic regression to estimate the denominator of the weights. I.e., the coditional probability of treatment given confounders $L$ 
3) Estimate $W^A$ using the formula above
4) Lastly, fit a weighted regression model to estimate the treatment effect. Because IP weighting must be taken into account for standard errors to be correct, we use a method which provides robust 'sandwhich' type standard errors (bootstrapping is an alternative option). 

Provided identifiability conditions are satisfied, the model fitted to the pseudo-population created in step 4 has the form: $$E[Y^a]=\beta_0 + \beta_1 a$$ 
This is referred to as a *marginal structural model*. It is marginal because the outcome is a marginal quantity & structural as it is a model for a *counterfactual*, rather than fully observed, outcome.


#### Applying IPT weighting ####

The code below performs the four steps above:

```{r create weights}

library(geepack) # for robust standard errors

# fit model for numerator (step 1)

fit_num <- glm(qsmk ~ 1, data = nhefs, family = binomial())

numerator <- ifelse(nhefs$qsmk == 1, 
                    predict(fit_num, type = "response"),
                    1 - predict(fit_num, type = "response"))

# fit model for denominator (step 2)

fit_denom <- glm(qsmk ~ sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) + exercise + active + pol(wt71), 
            family = binomial(), data = nhefs)

denominator <- ifelse(nhefs$qsmk == 1,
                      predict(fit_denom, type = "response"),
                      1 - predict(fit_denom, type = "response"))

# calculate weights (setp 3)

ipt_weights <- numerator / denominator 

# fit marginal structural model (step 4)

ip_mod <- geeglm(wt82_71 ~ qsmk, data = nhefs, weights = ipt_weights,
                 id = seqn, corstr = "independence")

# inference 

beta <- round(coef(ip_mod),2)
SE <- coef(summary(ip_mod))[, 2]
lcl <- round(beta - 1.96 * SE,2)
ucl <- round(beta + 1.96 * SE,2)
cbind(beta, lcl, ucl)

```

The IPTW estimate for smoking cessation is a 3.44 kg increase in weight gain, with a 95% CI ranging from 2.4 to 4.5. Very similar to that obtained by the parametric g-formula. This is a reassuring sign as these two methods rely on different modeling assumptions, as will be touched on later.

#### Checking balance ####

The central idea between IPT weighting is that, in the pseudo-population produced by the weights, treatment and confounders are statistically independent. As discussed in the paper by [Austin & Stuart (2015)](https://onlinelibrary.wiley.com/doi/10.1002/sim.6607), it is best practise to assess that the estimated weights are working as they should and that measured confounders are balanced across levels of treatment. This can be easily checked using The bal.tab function of the [cobalt]("https://ngreifer.github.io/cobalt/index.html") package. In our case, these results (not shown) indicate that measured confounders are well balanced across treatment in the pseudo-population. 



### Doubly robust estimation ###

As mentioned previously, key assumptions of the parametric g-formula and IPTW estimators are that the models for the outcome and treatment, respectively, are correctly specified. Doubly robust (DR) estimators, on the other hand, combine models for both treatment and the outcome and by doing so, give the analyst *two chances* to get the model right. That is, a DR estimator will be consistent so long as *at least one* of the outcome or treatment models are correctly specified. And, as a bonus, if both models are correct, the DR estimator will have a smaller variance than the IPTW estimator. 

We will use the DR estimator of [Bang and Robins (2005)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x) as an example. The steps are as follows:

1) Estimate the probability of treatment $A$ given confounders $L$ using a logistic regression model
2) Compute the 'clever covariate' $R$ which equals $1/Pr[A=1|L=l]$ when treatment is given and $1/-(1-Pr[A|L=l])$ when treatment is not given 
3) Fit a regression model for the conditional mean of the outcome, including clever covariate $R$ as a predictor
4) Standardisation as per the g-formula method above. Note, in the artificial dataset where treatment is given ($A=1$), the clever covariate $R$ should take value $1/Pr[A=1|L=l]$. In the artificial dataset where treatment is not given ($A=0$), $R$ takes value $1/-(1-Pr[A|L=l])$

#### Applying the doubly robust estimator ####

And here is the code to apply the estimator to the smoking cessation data (thank you to Noah Greifer for finding and correcting an earlier [error](https://stats.stackexchange.com/questions/615519/bang-and-robins-doubly-robust-estimator-biased-and-with-large-variance/615583?noredirect=1#comment1143991_615583) in my code):

```{r}

# fit model for treatment

treatment_mod <- glm(qsmk ~ sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) + exercise + active + pol(wt71), 
                       family = binomial(), data = nhefs)

prob_treatment <- predict(treatment_mod, type = "response")

# Create 'clever covariate' 

R <- ifelse(nhefs$qsmk == 1,
            1 / prob_treatment,
            1 / -(1 - prob_treatment))

# Fit outcome model including clever covariate 

outcome_mod <- lm(wt82_71 ~ qsmk + sex + race + pol(age) + education + pol(smokeintensity) + pol(smokeyrs) + exercise + active + pol(wt71) + qsmk:smokeintensity + R, data = nhefs)

# standardise 

nhefs.0 <- nhefs.1 <- nhefs

nhefs.0$qsmk <- 0
nhefs.0$R <- 1 / -(1 - prob_treatment)

nhefs.1$qsmk <- 1
nhefs.1$R <- 1 / prob_treatment

E_0 <- mean(predict(outcome_mod, newdata = nhefs.0))

E_1 <- mean(predict(outcome_mod, newdata = nhefs.1))

treat.effect <- E_1 - E_0

print(paste("E[Y(1)] - E[Y(0)] = ",round(treat.effect,2)))

```

The DR estimate is very similar to those of the g-formula and IPTW estimators. Confidence intervals could be obtained by bootstrapping. 


### Targeted maximum likelihood estimation (TMLE) ###

All of the above approaches for the estimation of causal effects have made use of standard parametric models for estimation (that is, linear and logistic regression). Because these models tend to impose restrictions on the manner in which covariates are related to the outcome (e.g., no or few interactions, linearity of continuous variables), some amount of model mispecification is very likely in real settings. By contrast, highly flexible machine learning models impose few such restrictions and may be less prone to misspecification related bias. 

TMLE is a procedure for the doubly robust estimation of causal effects which makes use of machine learning tools. The basic mechanics, as outlined by [Luque-Fernandez et al (2018)](https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7628), are similar to that of the Bang and Robins estimator, though a little more involved. The full procedure will be explored in more detail in a later post. For now, we'll go straight to estimating the effect of smoking cessation using the R tmle package. 

Models for the conditional probability of treatment and the conditional mean on the outcome, given confounders $L$, will be estimated using [SuperLearner](https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html#background). SuperLearner is an algorithm which uses several machine learning tools, estimates their performance using cross-validation, and then creates an optimally weighted average of those models (an "ensemble"). Here, we'll make use of generalised linear models, penalised maximum likelihood (glmnet), generalised additive models, and XGBoost in the SuperLearner algorithm. 


```{r tmle}

suppressPackageStartupMessages(library(SuperLearner))
suppressPackageStartupMessages(library(tmle))

# data.frame of confounder variables

confounds <- select(nhefs, sex, race, age, education, smokeintensity, smokeyrs, exercise, active, wt71)

SL.library = c("SL.glm", 
               "SL.glmnet",
               "SL.gam",
               "SL.xgboost") 

tmle.out <- tmle(Y = nhefs$wt82_71, 
                 A = nhefs$qsmk, 
                 W = confounds, 
                 family = "gaussian", 
                 V = 5,
                 Q.SL.library = SL.library, 
                 g.SL.library = SL.library)

tmle.out

```

The estimate and confidence interval are very similar to those from previous methods. Which might indicate that our parameteric models were reasonably well-specified, though it remains uncertain whether other conditions (such as no unmeasured confounding) are satisfied. 


### References ###

Hernán, M. A., & Robins, J. M. (2010). Causal inference. 

Arel-Bundock V (2023). marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests. R package version 0.9.0, https://vincentarelbundock.github.io/marginaleffects/.

Greifer N, Worthington S, Iacus S, King G (2023). clarify: Simulation-Based Inference for Regression Models. https://github.com/iqss/clarify, https://iqss.github.io/clarify/. 

Austin, P. C., & Stuart, E. A. (2015). Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies. Statistics in medicine, 34(28), 3661-3679.

Greifer N (2023). cobalt: Covariate Balance Tables and Plots. https://ngreifer.github.io/cobalt/, https://github.com/ngreifer/cobalt. 

Bang, H., & Robins, J. M. (2005). Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4), 962-973.

Luque‐Fernandez, M. A., Schomaker, M., Rachet, B., & Schnitzer, M. E. (2018). Targeted maximum likelihood estimation for a binary treatment: A tutorial. Statistics in medicine, 37(16), 2530-2546.
